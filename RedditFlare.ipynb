{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedditFlare.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIjNv3bKyUN+957iXLpKvu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yETe0VewEsD",
        "colab_type": "code",
        "outputId": "4758619c-6121-466d-9d33-951a03a74ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "!pip install praw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c0/b9714b4fb164368843b41482a3cac11938021871adf99bf5aaa3980b0182/praw-6.5.1-py3-none-any.whl (134kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 3.5MB/s \n",
            "\u001b[?25hCollecting prawcore<2.0,>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/76/b5/ce6282dea45cba6f08a30e25d18e0f3d33277e2c9fcbda75644b8dc0089b/prawcore-1.0.1-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 8.9MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.16\n",
            "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from prawcore<2.0,>=1.0.1->praw) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (1.24.3)\n",
            "Installing collected packages: prawcore, websocket-client, update-checker, praw\n",
            "Successfully installed praw-6.5.1 prawcore-1.0.1 update-checker-0.16 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agSoYqGnrqcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import requests.auth\n",
        "import praw\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIu7BJe44jbB",
        "colab_type": "code",
        "outputId": "aca9ee85-bca1-409f-f973-8ec5b8c87c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYc_AinU6J1R",
        "colab_type": "text"
      },
      "source": [
        "Setting up the reddit api to get details from various posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL69VhW7r3G4",
        "colab_type": "code",
        "outputId": "c1280418-f3a9-42ae-dca3-c36be378f915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "client_auth = requests.auth.HTTPBasicAuth(@Client_id, @Client_secret)\n",
        "post_data = {\"grant_type\": \"password\", \"username\": @username, \"password\": @password}\n",
        "headers = {\"User-Agent\": \"reddit_scraper\"}\n",
        "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
        "response.json()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'access_token': '284251727896-icg7SJe04MmHmMmMa5OpWlAIzo0',\n",
              " 'expires_in': 3600,\n",
              " 'scope': '*',\n",
              " 'token_type': 'bearer'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWF27GAwvd6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit = praw.Reddit(client_id=@Client_id, client_secret=@Client_secret, user_agent='reddit_scraper')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3YmRGPd6oFk",
        "colab_type": "text"
      },
      "source": [
        "Getting the top 100 posts and top 10 comments from r/india habung the flairs mentioned below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hvG9kWfwUo0",
        "colab_type": "code",
        "outputId": "5ace1e62-0f72-463e-a133-a58a47bf188e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "subreddit = reddit.subreddit('india')\n",
        "flairs = [\"AskIndia\", \"Non-Political\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\",\"Sports\", \"Food\", \"AMA\"]\n",
        "posts = []\n",
        "for i in flairs:\n",
        "  red = subreddit.search(f\"flair_name:{i}\",limit=100)\n",
        "  for post in red:\n",
        "    temp = []\n",
        "    temp.extend((post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created, i))\n",
        "    post.comments.replace_more(limit=0)\n",
        "    j=0\n",
        "    for comment in post.comments.list():\n",
        "      if j==10:\n",
        "        temp.append(cmt)\n",
        "        break\n",
        "      if j == 0:\n",
        "        cmt = str(comment.body)\n",
        "      else:\n",
        "        cmt = cmt + \" \" + str(comment.body)\n",
        "      j += 1\n",
        "    posts.append(temp)\n",
        "print(posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwXpsagFZ-kL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postsdf = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created', 'flair', 'comments'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ni_qDu25lZS",
        "colab_type": "code",
        "outputId": "c090c2d6-9a10-4f5b-dd51-e8cf0f31476c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "postsdf.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1100 entries, 0 to 1099\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   title         1100 non-null   object \n",
            " 1   score         1100 non-null   int64  \n",
            " 2   id            1100 non-null   object \n",
            " 3   subreddit     1100 non-null   object \n",
            " 4   url           1100 non-null   object \n",
            " 5   num_comments  1100 non-null   int64  \n",
            " 6   body          1100 non-null   object \n",
            " 7   created       1100 non-null   float64\n",
            " 8   flair         1100 non-null   object \n",
            " 9   comments      846 non-null    object \n",
            "dtypes: float64(1), int64(2), object(7)\n",
            "memory usage: 86.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbRZhfso64ua",
        "colab_type": "text"
      },
      "source": [
        "Cleaning the text removing punctuationas and stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0RIMyyn4T7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r1 = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "r2 = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = r1.sub(' ', text) # replace certain symbols by space in text\n",
        "    text = r2.sub('', text) # delete symbols from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove STOPWORDS from text\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DA6qeqI7fYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postsdf['title'] = postsdf['title'].apply(str)\n",
        "postsdf['body'] = postsdf['body'].apply(str)\n",
        "postsdf['comments'] = postsdf['comments'].apply(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XGTL7rV4npP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postsdf['title'] = postsdf['title'].apply(clean_text)\n",
        "postsdf['body'] = postsdf['body'].apply(clean_text)\n",
        "postsdf['comments'] = postsdf['comments'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISWbKUpB8Jhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postsdf.to_csv(\"data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLZVdALK-dvs",
        "colab_type": "text"
      },
      "source": [
        "MODELS\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw7NsyIrHkNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import svm\n",
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvDUBzxM-c9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/data.csv')\n",
        "flairs = [\"AskIndia\", \"Non-Political\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\",\"Sports\", \"Food\", \"AMA\"]\n",
        "df.fillna(\"\",inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_qZnsdD7E6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Creating various features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxJd-ijyjDiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1 = df['title']\n",
        "f2 = df['comments']\n",
        "f3 = df['url']\n",
        "f4 = df['body']\n",
        "f5 = df['title'] +\" \"+ df['comments'] +\" \"+ df['url'] +\" \"+ df['body']\n",
        "f6 = df['title'] +\" \"+ df['comments'] +\" \"+ df['url']\n",
        "fl = df['flair']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVwMr0K17Am6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Creating models to train on the features which were made"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1n04jzZHqWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runModel(data, fl):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(data, fl, test_size=0.2, random_state = 42)\n",
        "  vect = CountVectorizer()\n",
        "  tfidf = TfidfTransformer()\n",
        "  models = [LogisticRegression(n_jobs=1, C=1e5, max_iter=4000),\n",
        "            svm.SVC(), \n",
        "            MultinomialNB(), \n",
        "            RandomForestClassifier(n_estimators = 1000, random_state = 42), \n",
        "            xgb.XGBClassifier(), \n",
        "            MLPClassifier(hidden_layer_sizes=(30,30,30)), \n",
        "            SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)]\n",
        "  pred = []\n",
        "  acc = []\n",
        "  for i in models:\n",
        "    pipeline = Pipeline([('vect',vect), ('tfidf',tfidf), ('clf',i)])\n",
        "    pipeline.fit(x_train, y_train)\n",
        "    temp = pipeline.predict(x_test)\n",
        "    pred.append(temp)\n",
        "    acc.append(accuracy_score(temp, y_test))\n",
        "  return pred, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppTnr2L07HSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Finding the best model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DGKaF8Wkx9t",
        "colab_type": "code",
        "outputId": "8cf9a170-711b-4803-c15d-cc93019ab133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a1,b1 = runModel(f1,fl)\n",
        "print(max(b1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5409090909090909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BjovJ3k6jb",
        "colab_type": "code",
        "outputId": "5a6d22dc-c733-4609-e27f-3968a865c116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a2,b2 = runModel(f2,fl)\n",
        "print(max(b2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5227272727272727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3dCZ97_k7QC",
        "colab_type": "code",
        "outputId": "302b6461-b2ff-47b9-fc57-446133ed950d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a3,b3 = runModel(f3,fl)\n",
        "print(max(b3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.42727272727272725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttgg8JT0k9B6",
        "colab_type": "code",
        "outputId": "65dd2724-a276-4751-f11f-8d994bc61822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a4,b4 = runModel(f4,fl)\n",
        "print(max(b1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5409090909090909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG5LKgl6k8sZ",
        "colab_type": "code",
        "outputId": "71c9e021-843e-417a-98da-bc76e67dbb70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a5,b5 = runModel(f5,fl)\n",
        "print(max(b5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6909090909090909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cVaL6Sg7PKS",
        "colab_type": "text"
      },
      "source": [
        "Converting the best model into a pickle file to put on flask server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A5a8Cf8xphb",
        "colab_type": "code",
        "outputId": "02838722-1827-4179-8b67-8d8cdc731643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(f5, fl, test_size=0.2, random_state = 42)\n",
        "vect = CountVectorizer()\n",
        "tfidf = TfidfTransformer()\n",
        "pipeline = Pipeline([('vect',vect), ('tfidf',tfidf), ('clf',LogisticRegression(n_jobs=1, C=1e5, max_iter=4000))])\n",
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=4000,\n",
              "                                    multi_class='auto', n_jobs=1, penalty='l2',\n",
              "                                    random_state=None, solver='lbfgs',\n",
              "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gsVafwjxmmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnFyO7kixofl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(pipeline,open(\"model.pkl\",'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}